{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hamx0aGRP-A6"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4745,
     "status": "ok",
     "timestamp": 1640253838050,
     "user": {
      "displayName": "Projet HDDL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15092415087239541407"
     },
     "user_tz": -60
    },
    "id": "Lae6eA-0QSrH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\celia\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets, metrics, model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "# from plot_keras_history import plot_history\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "import sys\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from missingpy import MissForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CbPT7ioQphP"
   },
   "source": [
    "# Pré-traitement train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12176,
     "status": "ok",
     "timestamp": 1640253850205,
     "user": {
      "displayName": "Projet HDDL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15092415087239541407"
     },
     "user_tz": -60
    },
    "id": "RgVHsoRsQtCG"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"X_station_train.csv\",parse_dates=['date'],infer_datetime_format=True)\n",
    "coords   = pd.read_csv(\"stations_coordinates.csv\")\n",
    "Y_train  = pd.read_csv(\"Y_train.csv\",parse_dates=['date'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On clip les outliers\n",
    "def find_outliers(series):\n",
    "    return (series - series.mean()) > 1.5 * series.std() # ou 2.4\n",
    "\n",
    "\n",
    "def cap_values(series):\n",
    "    outliers = find_outliers(series)\n",
    "    max_val = series[~outliers].max()\n",
    "    print(max_val)\n",
    "    series[outliers] = max_val\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_train(coords_df, train, train_Y):\n",
    "    # Merge coords + train \n",
    "    coords_df['number_sta'] = coords_df['number_sta'].astype('category')\n",
    "    df_X_train = train.merge(coords, on=['number_sta'], how='left')\n",
    "    \n",
    "    # Traitement date\n",
    "    df_X_train[\"date_wh\"] = df_X_train[\"date\"].apply(lambda x: dt.date(x.year, x.month, x.day))\n",
    "    \n",
    "    # Traitement NaN\n",
    "    # Premier remplissage par station et date\n",
    "    df_X_train = df_X_train.groupby(['number_sta','date'], sort=False).apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "    # Remplissage des données manquantes restantes par l'algorithme MissForest\n",
    "    #imputation_train = MissForest()\n",
    "    #df_X_train = imputation_train.fit_transform(df_X_train)\n",
    "    \n",
    "    # Moyenne des variables groupées par la date et la station\n",
    "    sub_df1 = df_X_train[['date_wh', 'number_sta',\"ff\", \"t\", \"td\", \"hu\", \"dd\"]].groupby(['date_wh', 'number_sta']).mean().reset_index()\n",
    "\n",
    "    # Somme des précipitations sur la date et la station\n",
    "    sub_df2 = df_X_train[['date_wh', 'number_sta',\"precip\"]].groupby(['date_wh', 'number_sta']).sum().reset_index()\n",
    "\n",
    "    # Récupération des latitudes, logitudes, hauteurs des stations \n",
    "    sub_df3 = df_X_train[['date_wh', 'number_sta',\"lat\", \"lon\", \"height_sta\"]].drop_duplicates(['date_wh', 'number_sta'])\n",
    "\n",
    "    # Fusion des 3 sub_sets\n",
    "    df_X_train = sub_df1.merge(sub_df2, on = [\"date_wh\", \"number_sta\" ], how = \"left\")\n",
    "    df_X_train = df_X_train.merge(sub_df3, on = [\"date_wh\", \"number_sta\" ], how = \"left\")\n",
    "    df_X_train = df_X_train.rename(columns = {\"date_wh\": \"date\"})\n",
    "    df_X_train.reset_index()\n",
    "    \n",
    "    # Ajout colonne Y_train\n",
    "    train_Y['number_sta'] = train_Y['number_sta'].astype('category')\n",
    "    df_X_train['number_sta'] = df_X_train['number_sta'].astype('category')\n",
    "    train_Y[\"date\"] = train_Y[\"date\"].apply(lambda x: x- dt.timedelta(days=1))\n",
    "    df_X_train['date'] = df_X_train['date'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))\n",
    "    df_X_train = pd.merge(df_X_train, train_Y, on = [\"number_sta\", \"date\"], how = \"left\")\n",
    "    \n",
    "    # Ajout du mois \n",
    "    df_X_train[\"month\"] = df_X_train[\"date\"].apply(lambda x: x.month)\n",
    "    # Transformation en variable facteur\n",
    "    df_X_train['month'] = pd.Categorical(df_X_train['month'], ordered=False)\n",
    "    \n",
    "    # Gestion des données manquantes de la valeur à prédire\n",
    "    df_X_train.dropna(inplace = True) # on supprime les lignes dont la valeur exacte à prédire est manquante\n",
    "    df_X_train.drop([\"date\", \"number_sta\", \"Id\"], axis = 1, inplace = True)\n",
    "    \n",
    "    # Variable month \n",
    "    df_X_trainDum = pd.get_dummies(df_X_train[[\"month\"]])\n",
    "    del df_X_trainDum[\"month_1\"] \n",
    "    df_X_train.drop('month', axis=1, inplace=True)\n",
    "    df_X_train =pd.concat([df_X_train, df_X_trainDum,],axis=1)\n",
    "    \n",
    "    # Clip values\n",
    "    capped_values = cap_values(df_X_train[\"Ground_truth\"])\n",
    "    df_X_train.drop([\"Ground_truth\"], axis = 1, inplace=True)\n",
    "    df_X_train[\"Ground_truth\"] = capped_values\n",
    "    \n",
    "    return df_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = treat_train(coords, df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.to_csv(\"df_train_nan_by_station_date.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCXZ_Nkfe2mt"
   },
   "source": [
    "# Pré-traitement test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6014,
     "status": "ok",
     "timestamp": 1640253856187,
     "user": {
      "displayName": "Projet HDDL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15092415087239541407"
     },
     "user_tz": -60
    },
    "id": "LJq8cDzHe52X"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_test = pd.read_csv(\"X_station_test.csv\")\n",
    "#test_forecast = pd.read_csv(\"Baseline_forecast_test.csv\")\n",
    "test_obs = pd.read_csv(\"Baseline_observation_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_test(test_df, obs_test, coords_df):\n",
    "    # Number_sta + merge coords\n",
    "    df_X_test = test_df.copy() \n",
    "    coords_df['number_sta'] = coords_df['number_sta'].astype('category')\n",
    "    df_X_test[\"number_sta\"] = df_X_test[\"Id\"].apply(lambda x : x.split(\"_\")[0])\n",
    "    df_X_test[\"number_sta\"] = df_X_test[\"number_sta\"].astype(\"int\")\n",
    "    df_X_test = df_X_test.merge(coords_df, on=[\"number_sta\"], how = \"left\")\n",
    "    \n",
    "    # Création variable day \n",
    "    df_day = df_X_test[\"Id\"].apply(lambda x : x.split(\"_\")[1])\n",
    "    df_X_test.insert(0,\"day\", df_day)\n",
    "    df_X_test[\"day\"] = df_X_test[\"day\"].astype(\"int\")\n",
    "    # test -> changement en facteur\n",
    "    df_X_test['day'] = pd.Categorical(df_X_test['day'], ordered=False)\n",
    "    \n",
    "    # Sort by number_sta and day \n",
    "    df_X_test.sort_values(by = ['number_sta', 'day'], inplace = True)\n",
    "    \n",
    "    # Fill na\n",
    "    df_X_test = df_X_test.groupby(['number_sta','day'], sort=False).apply(lambda x: x.ffill().bfill())\n",
    "    #imputation = MissForest()\n",
    "    #df_X_test = imputation.fit_transform(df_X_test)\n",
    "    \n",
    "    # Moyenne des variables groupées par la date et la station\n",
    "    sub_df1 = df_X_test[['day', 'number_sta',\"ff\", \"t\", \"td\", \"hu\", \"dd\"]].groupby(['day', 'number_sta']).mean().reset_index()\n",
    "\n",
    "    # Somme des précipitations sur la date et la station\n",
    "    sub_df2 = df_X_test[['day', 'number_sta',\"precip\"]].groupby(['day', 'number_sta']).sum().reset_index()\n",
    "\n",
    "    # Récupération des latitudes, logitudes, hauteurs des stations, du mois et de l'Id\n",
    "    sub_df3 = df_X_test[['day', 'number_sta',\"lat\", \"lon\", \"height_sta\", \"month\", \"Id\"]].drop_duplicates(['day', 'number_sta'])\n",
    "\n",
    "    # Fusion des 3 sub_sets\n",
    "    df_X_test = sub_df1.merge(sub_df2, on = [\"day\", \"number_sta\" ], how = \"left\")\n",
    "    df_X_test = df_X_test.merge(sub_df3, on = [\"day\", \"number_sta\" ], how = \"left\")\n",
    "    df_X_test = df_X_test.rename(columns = {\"day\": \"date\"})\n",
    "    df_X_test.reset_index()\n",
    "\n",
    "    # Sauvegarde des Ids sous le bon format pour plus tard\n",
    "    id_list = pd.DataFrame(data = df_X_test[\"Id\"].apply(lambda x : x.split(\"_\")[0] + '_' + x.split(\"_\")[1]))\n",
    "    df_X_test.drop(['Id'], axis=1, inplace=True)\n",
    "    df_X_test['Id'] = id_list\n",
    "\n",
    "    # Changement du type de la variable month en facteur \n",
    "    df_X_test['month'] = pd.Categorical(df_X_test['month'], ordered=False)\n",
    "\n",
    "    df_X_testDum = pd.get_dummies(df_X_test[[\"month\"]])\n",
    "\n",
    "    del df_X_testDum[\"month_1\"] \n",
    "\n",
    "    # Variables explicatives quantitatives\n",
    "    df_X_testQuant= df_X_test.drop('month', axis=1, inplace=False)\n",
    "\n",
    "    # Variables explicatives\n",
    "    df_X_test = pd.concat([df_X_testQuant, df_X_testDum,],axis=1)\n",
    "    \n",
    "    # On ne conserve que les Id qui sont dans la liste de la baseline (Id souhaités pour la prédiction)\n",
    "    df_X_test = df_X_test[df_X_test.Id.isin(test_obs['Id'])]\n",
    "    \n",
    "    return df_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test = pre_test(df_test, test_obs, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test.to_csv(\"df_test_nan_by_station_day.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMiLupeic+54IfYeWDuMQeE",
   "collapsed_sections": [],
   "mount_file_id": "1Sz4453CgFJbU9zK-zxKJAKqSSomVfmQi",
   "name": "test_new_treatment_df_test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
